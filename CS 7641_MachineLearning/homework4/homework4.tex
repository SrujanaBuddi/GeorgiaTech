\documentclass[twoside,11pt]{article}\usepackage{amsmath,amsfonts,amsthm,fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\setlength{\parindent}{0pt}
\usepackage{graphicx}
\usepackage{bm}
\def\argmin{\operatornamewithlimits{arg\, min}}
\newcommand{\rbr}[1]{\left(#1\right)}
\newcommand{\cbr}[1]{\left\{#1\right\}}
\newcommand{\Ncal}{\mathcal{N}}
\begin{document}

\title{CS 7641 CSE/ISYE 6740 Homework 4}
\author{Le Song}
\date{Deadline: 11/28 Monday, 11:55 pm}
\maketitle

\begin{itemize}
  \item Submit your answers as an electronic copy on T-square.
  \item No unapproved extension of deadline is allowed. Late
  submission will lead to 0 credit.
  \item Typing with Latex is highly recommended. Typing with MS Word is also okay.
  If you handwrite, try to be clear as much as possible. No credit may be given to unreadable handwriting.
  \item Explicitly mention your collaborators if any. For the programming problem, it is
  absolutely not allowed to share your source code with anyone in the
  class as well as to use code from the Internet without reference.
  \item Recommended reading: PRML Section 13.2
 \end{itemize}
 

\section{Kernels [20 points]}
\subsubsection*{(a) Identify which of the followings is a valid kernel.  If it is a kernel, please write your answer explicitly as `True' and give mathematical proofs. If it is not a kernel, please write your answer explicitly as `False' and give explanations. [8 pts]}
 Suppose $K_1$ and $K_2$ are valid kernels (symmetric and positive definite) defined on $R^m\times R^m$.
\begin{enumerate}
\item $K(u,v) = \alpha K_1(u,v) + \beta K_2(u,v), \alpha,\beta\in R$.
\item $K(u,v) = K_1(f(u), f(v))$ where $f:R^m \rightarrow R^m$.
coefficients.
\item \begin{align}
	K(u,v) = \left\{
	\begin{aligned}
	&1 & \text{if } \|u-v\|_2 \leqslant 1\\
	& 0 & \text{otherwise}
	\end{aligned}
	\right.
	\end{align}
	
\item Suppose $K'$ is a valid kernel. 
	\begin{align}
	K(u,v) = \frac{K'(u,v)}{\sqrt{K'(u,u)K'(v,v)}}.
	\end{align}
	
\end{enumerate} 
 
\subsubsection*{(b) Write down kernelized version of Fisher's Linear Discriminant Analysis using kernel trick. Please provide full steps and all details of the method. [\emph{Hint: Use kernel to replace inner products.}] [12 pts]}
 
 

\section{Markov Random Field, Conditional Random Field [20 pts]}

\textbf{[a-b]} A probability distribution on  3 discrete variables
a,b,c is defined by $P(a,b,c) = \frac{1}{Z}\psi(a,b,c) =
\frac{1}{Z}\phi_1(a,b)\phi_2(b,c)$, where the table for the two
factors are given below.

\begin{table}[!htb]

    \begin{minipage}{.5\linewidth}

      \centering

    \begin{tabular}{cc|c}
    a & b & $\phi_1(a,b)$ \\ \hline
    0 & 0 & 4             \\
    0 & 1 & 3             \\
    1 & 0 & 3             \\
    1 & 1 & 1             \\
    \end{tabular}

    \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \centering
     \begin{tabular}{cc|c}
    b & c & $\phi_2(b,c)$ \\ \hline
    0 & 0 & 3             \\
    0 & 1 & 2             \\
    0 & 2 & 1             \\
    1 & 0 & 4             \\
    1 & 1 & 1             \\
    1 & 2 & 3             \\
    \end{tabular}
    \end{minipage}
\end{table}

\subsubsection*{(a) Compute the slice of the joint factor $\psi(a,b,c)$ corresponding to $b = 1$. This is the table $\psi(a,b=1,c)$. [5 pts]}

\subsubsection*{(b) Compute $P(a = 1,b = 1)$. [5 pts]}

\subsubsection*{(c) Explain the difference between Conditional Random Fields and Hidden
Markov Models with respect to the following factors. Please give
only a one-line explanation. [10 pts]}

\begin{itemize}
  \item Type of model - generative/discriminative
  \item Objective function optimized
  \item Require a normalization constant
\end{itemize}

\section{Hidden Markov Model [50 pts]}

This problem will let you get familiar with HMM algorithms by doing
the calculations by hand.

\textbf{[a-c]} There are three coins $(1,2,3)$, to throw them
randomly, and record the result. $S = {1,2,3}$; $V = {H,T}$ (Head or
Tail); $A, B, \pi$ is given as

\begin{table}[!htb]
    \begin{minipage}{.5\linewidth}
      \centering
A:
    \begin{tabular}{l|l|l|l}
    ~ & 1    & 2    & 3    \\\hline
    1 & 0.9  & 0.05 & 0.05 \\
    2 & 0.45 & 0.1  & 0.45 \\
    3 & 0.45 & 0.45 & 0.1  \\
    \end{tabular}

    \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \centering

B:
    \begin{tabular}{l|l|l|l}
    ~ & 1   & 2    & 3    \\\hline
    H & 0.5 & 0.75 & 0.25 \\
    T & 0.5 & 0.25 & 0.75 \\
    \end{tabular}

    \end{minipage}

    \begin{minipage}{.5\linewidth}
      \centering
$\pi:$
    \begin{tabular}{llll}
    $\pi$ & 1/3 & 1/3 & 1/3 \\
    \end{tabular}
  \end{minipage}

\end{table}

\subsubsection*{(a) Given the model above, what's the probability of observation $O = {H,T,H}$. [10 pts]}

\subsubsection*{(b) Describe how to get the $A, B$, and $\pi$, when they are unknown. [10 pts]}


\subsubsection*{(c) In class, we studied discrete HMMs with discrete hidden states and
observations. The following problem considers a continuous density
HMM, which has discrete hidden states but continuous observations.
Let $S_t \in {1, 2, ..., n}$ denote the hidden state of the HMM at
time t, and let $X_t \in R$ denote the real-valued scalar
observation of the HMM at time t. In a continuous density HMM, the
emission probability must be parameterized since the random variable
$X_t$ is no longer discrete. It is defined as $P(X_t = x|S_t = i) =
\mathcal{N}(\mu_i,\sigma_i^2)$. Given $m$ sequences of observations
(each of length $T$), derive the EM algorithm for HMM with Gaussian
observation model. [14 pts]}


\subsubsection*{(d) For each of the following sentences, say whether it is true or false and provide a short explanation (one
sentence or so). [16 pts]}

\begin{itemize}
  \item The weights of all incoming edges to a state of an HMM must sum to 1.
  \item An edge from state $s$ to state $t$ in an HMM denotes the conditional probability of going to state s given that we are currently at state $t$.
  \item The "Markov" property of an HMM implies that we cannot use an HMM to model a process that depends on several time-steps in the past.
  \item The Baum-Welch algorithm is a type of an Expectation Maximization algorithm and as such it is guaranteed to converge to the (globally) optimal solution.
\end{itemize}


%\newpage
\section{Programming [30 pts]}

In this problem, you will implement  algorithm to analyze the
behavior of \emph{SP500} index over a period of time. For each week,
we measure the price movement relative to the previous week and
denote it using a binary variable (+1 indicates up and 1 indicates
down). The price movements from week 1 (the week of January 5) to
week 39 (the week of September 28) are plotted below.

Consider a Hidden Markov Model in which $x_t$ denotes the economic
state (good or bad) of week t and $y_t$ denotes the price movement
(up or down) of the \emph{SP500} index. We assume that
$x_{(t+1)}=x_t$ with probability 0.8, and
$P_{(Y_t|X_t)}(y_t=+1|x_t=\text{good}) =
P_{(Y_t|X_t)}(y_t=-1|x_t=\text{bad}) = q.$ In addition, assume that
$P_{(X_1)}(x_1=\text{bad}) = 0.8$. Load the \texttt{sp500.mat},
implement the algorithm, briefly describe how you implement this and
report the following :

\subsubsection*{(a) Assuming $q = 0.7$, plot $P_{(X_t|Y)}(x_t = \text{good}|y)$ for $t = 1,2,...,39$. What is the probability that the economy is in a good state in the week of week 39. [15 pts]}

\subsubsection*{(b) Repeat (a) for $q = 0.9$, and compare the result to that of (a). Explain your comparison in one or two sentences. [15 pts]}


\end{document}
